{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3C6WuWBXwjKO/pCqIT1Hu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akhil27/5731_project/blob/main/INFO_5731_GROUP4_PROJECT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VVzUkrLxorG",
        "outputId": "cbc657bf-bd11-4838-891d-5653f8ff6e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install tesseract-ocr\n",
        "!pip install pytesseract\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "063HuuW2YrIM",
        "outputId": "7618537e-f6e0-41f4-fcfd-07afc6f088fb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.2.1)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"Pamphlet.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Pamphlet\")):\n",
        "    base_folder = os.path.join(extract_path, \"Pamphlet\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: 17th Annual Country Club Golf Tournament [Flyer]\n",
        "- Creator: Lopez, J. Lewis (Tournament Chairman)\n",
        "- Subject: Golf tournaments, country clubs, leisure activities, Galveston\n",
        "- Description: A flyer providing a schedule for the 17th annual Country Club Golf Tournament held in Galveston, Texas. It includes dates, social events, and organizational information associated with the event.\n",
        "- Publisher: Rosenberg Library\n",
        "- Contributor: Kempner, Harris Leon (Correspondent)\n",
        "- Date: 1960-08-11/1960-08-14\n",
        "- Type: Text\n",
        "- Format: Pamphlet; [1] page, 26 x 21 cm; JPEG\n",
        "- Identifier: ark:/67531/metapth1420610\n",
        "- Source: Harris and Eliza Kempner Collection, MS 80-0002\n",
        "- Language: English\n",
        "- Relation: Part of the Personal Papers (MS 80-0002) series\n",
        "- Coverage: Galveston, Texas, United States\n",
        "- Rights: Public Domain\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: ABC Bulb and Seed Specials for Spring Planting\n",
        "- Creator: American Bulb Company\n",
        "- Subject: Cannas, lilies, seedlings, flowers, gardening, spring planting\n",
        "- Description: A single-page pamphlet promoting seasonal flower seeds and bulbs from the American Bulb Company. It features prices, illustrations of flowers, and is addressed to D. W. Kempner.\n",
        "- Publisher: Rosenberg Library\n",
        "- Contributor: Kempner, Daniel W. (Daniel Webster), 1877‚Äì1956\n",
        "- Date: 1950\n",
        "- Type: Text\n",
        "- Format: Pamphlet; 1 page; 16 x 23 cm; JPEG\n",
        "- Identifier: ark:/67531/metapth1349947\n",
        "- Source: Harris and Eliza Kempner Collection, MS 80-0002\n",
        "- Language: English\n",
        "- Relation: Part of the Personal Papers (MS 80-0002) series\n",
        "- Coverage: United States\n",
        "- Rights: Public Domain\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical pamphlets following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: pamphlet title, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-maverick:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"pamphlets_metadata_llama.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'pamphlets_metadata_llama.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "1g10kOlY3Qu6",
        "outputId": "5a05cffa-c0a9-4aa1-c76a-8262e25545ba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: Pamphlet1\n",
            "\n",
            "üìÇ Processing: Pamphlet10\n",
            "\n",
            "üìÇ Processing: Pamphlet11\n",
            "\n",
            "üìÇ Processing: Pamphlet12\n",
            "\n",
            "üìÇ Processing: Pamphlet13\n",
            "\n",
            "üìÇ Processing: Pamphlet14\n",
            "\n",
            "üìÇ Processing: Pamphlet15\n",
            "\n",
            "üìÇ Processing: Pamphlet16\n",
            "\n",
            "üìÇ Processing: Pamphlet17\n",
            "\n",
            "üìÇ Processing: Pamphlet18\n",
            "\n",
            "üìÇ Processing: Pamphlet19\n",
            "\n",
            "üìÇ Processing: Pamphlet2\n",
            "\n",
            "üìÇ Processing: Pamphlet20\n",
            "\n",
            "üìÇ Processing: Pamphlet21\n",
            "\n",
            "üìÇ Processing: Pamphlet22\n",
            "\n",
            "üìÇ Processing: Pamphlet23\n",
            "\n",
            "üìÇ Processing: Pamphlet24\n",
            "\n",
            "üìÇ Processing: Pamphlet25\n",
            "\n",
            "üìÇ Processing: Pamphlet26\n",
            "\n",
            "üìÇ Processing: Pamphlet27\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1d35877c-acd9-4ef8-8e11-e3f0e29edba9\", \"pamphlets_metadata_llama.csv\", 23100)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'pamphlets_metadata_llama.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"Letter.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Letter\")):\n",
        "    base_folder = os.path.join(extract_path, \"Letter\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: Letter from Will Clayton to H. Kempner, July 15, 1943\n",
        "- Creator: Clayton, Will L. (Author)\n",
        "- Contributor: Kempner, Harris L. (Recipient)\n",
        "- Date: 1943-07-15\n",
        "- Content Description: A letter discussing cotton transactions and financial arrangements addressed to Harris Kempner.\n",
        "- Subject and Keywords: business correspondence, cotton industry, financial matters\n",
        "- Coverage: United States - Texas - Galveston County - Galveston\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Letter from D. W. Kempner to Harris Kempner, March 25, 1940\n",
        "- Creator: Kempner, Daniel Webster (Author)\n",
        "- Contributor: Kempner, Harris L. (Recipient)\n",
        "- Date: 1940-03-25\n",
        "- Content Description: A letter about business operations and family financial matters sent from Daniel Webster Kempner to Harris Kempner.\n",
        "- Subject and Keywords: business affairs, family correspondence, financial management\n",
        "- Coverage: United States - Texas - Galveston County - Galveston\n",
        "\n",
        "Example 3:\n",
        "Metadata:\n",
        "- Title: Letter from Moore and McKinley to Harris Kempner, May 5, 1941\n",
        "- Creator: Moore and McKinley (Author)\n",
        "- Contributor: Kempner, Harris L. (Recipient)\n",
        "- Date: 1941-05-05\n",
        "- Content Description: A letter from Moore and McKinley firm regarding banking operations and cotton trade issues directed to Harris Kempner.\n",
        "- Subject and Keywords: business letters, banking, cotton trade\n",
        "- Coverage: United States - Texas - Galveston County - Galveston\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical letters following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: Letter from A to B, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-maverick:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"letters_metadata_llama.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'letters_metadata_llama.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "mJw1sNPM1HbZ",
        "outputId": "e8ee3071-9a1c-4179-d4b9-4a3db2e22197"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: Letter 1\n",
            "\n",
            "üìÇ Processing: Letter 10\n",
            "\n",
            "üìÇ Processing: Letter 11\n",
            "\n",
            "üìÇ Processing: Letter 12\n",
            "\n",
            "üìÇ Processing: Letter 13\n",
            "\n",
            "üìÇ Processing: Letter 14\n",
            "\n",
            "üìÇ Processing: Letter 15\n",
            "\n",
            "üìÇ Processing: Letter 16\n",
            "\n",
            "üìÇ Processing: Letter 17\n",
            "\n",
            "üìÇ Processing: Letter 18\n",
            "\n",
            "üìÇ Processing: Letter 19\n",
            "\n",
            "üìÇ Processing: Letter 2\n",
            "\n",
            "üìÇ Processing: Letter 20\n",
            "\n",
            "üìÇ Processing: Letter 21\n",
            "\n",
            "üìÇ Processing: Letter 22\n",
            "\n",
            "üìÇ Processing: Letter 23\n",
            "\n",
            "üìÇ Processing: Letter 24\n",
            "\n",
            "üìÇ Processing: Letter 25\n",
            "\n",
            "üìÇ Processing: Letter 26\n",
            "\n",
            "üìÇ Processing: Letter 27\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_efd0e8e5-f1b9-4ffb-b3b2-82e6e9608728\", \"letters_metadata_llama.csv\", 21096)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'letters_metadata_llama.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"Photographs.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Photographs\")):\n",
        "    base_folder = os.path.join(extract_path, \"Photographs\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: Photograph of a Child Sitting and Holding a Dog\n",
        "- Creator: Unknown\n",
        "- Subject and Keywords: children, pets, outdoor activities\n",
        "- Content Description: A black-and-white photograph showing a young child seated on the ground, smiling while holding a small dog, with a rustic background.\n",
        "- Date: 19XX\n",
        "- Coverage: United States\n",
        "- Format: 1 photograph: ; 8 x 8 cm\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Photograph of a Family Gathering Outdoors\n",
        "- Creator: Unknown\n",
        "- Subject and Keywords: family, group portraits, leisure activities\n",
        "- Content Description: A photograph capturing a family of five posing together in a grassy field, dressed in early 20th-century attire.\n",
        "- Date: 19XX\n",
        "- Coverage: Germany - Berlin - Berlin\n",
        "- Format: 1 photograph: 9 x 14 cm\n",
        "\n",
        "Example 3:\n",
        "Metadata:\n",
        "- Title: Homestead Buildings with Barn and Yard\n",
        "- Creator: Unknown\n",
        "- Subject and Keywords: agriculture, homesteads, barns\n",
        "- Content Description: A rural homestead photograph showing a central muddy ground surrounded by several wooden buildings, indicative of farming activity.\n",
        "- Date: 19XX\n",
        "- Coverage: United States\n",
        "- Format: 1 photograph: 13 x 18 cm\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical photohraphs following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: Photograph title, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-scout:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"photographs_metadata_llama.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'photographs_metadata_llama.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "XRs8eHDECawq",
        "outputId": "ea13181b-df04-4ba8-e0a8-501dad1a0980"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: Photograph1\n",
            "\n",
            "üìÇ Processing: Photograph10\n",
            "\n",
            "üìÇ Processing: Photograph11\n",
            "\n",
            "üìÇ Processing: Photograph12\n",
            "\n",
            "üìÇ Processing: Photograph13\n",
            "\n",
            "üìÇ Processing: Photograph14\n",
            "\n",
            "üìÇ Processing: Photograph15\n",
            "\n",
            "üìÇ Processing: Photograph16\n",
            "\n",
            "üìÇ Processing: Photograph17\n",
            "\n",
            "üìÇ Processing: Photograph18\n",
            "\n",
            "üìÇ Processing: Photograph19\n",
            "\n",
            "üìÇ Processing: Photograph2\n",
            "\n",
            "üìÇ Processing: Photograph20\n",
            "\n",
            "üìÇ Processing: Photograph21\n",
            "\n",
            "üìÇ Processing: Photograph22\n",
            "\n",
            "üìÇ Processing: Photograph23\n",
            "\n",
            "üìÇ Processing: Photograph24\n",
            "\n",
            "üìÇ Processing: Photograph25\n",
            "\n",
            "üìÇ Processing: Photograph26\n",
            "\n",
            "üìÇ Processing: Photograph27\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1827f3cf-85f1-4c1f-83b4-72abd0a00cbf\", \"photographs_metadata_llama.csv\", 8420)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'photographs_metadata_llama.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"Image.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Image\")):\n",
        "    base_folder = os.path.join(extract_path, \"Image\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: Advertisement for Amana Air Conditioner\n",
        "- Creator: Amana Refrigeration Incorporated\n",
        "- Subject and Keywords: air conditioning, advertisements, consumer products\n",
        "- Content Description: A promotional pamphlet detailing technical specifications and product features for Amana brand residential air conditioners.\n",
        "- Date: 1955~\n",
        "- Coverage: United States - Iowa - Iowa County - Middle Amana\n",
        "- Format: 1 page; 28 x 22 cm\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Sugar Chart for the Year 1952\n",
        "- Creator: H. H. Pike & Son\n",
        "- Subject and Keywords: sugar trade, historical charts, commerce\n",
        "- Content Description: A commercial sugar chart showing commodity prices and trends for the year 1952, intended for businesses and traders.\n",
        "- Date: 1952\n",
        "- Coverage: United States\n",
        "- Format: 1 page; 36 x 49 cm\n",
        "\n",
        "Example 3:\n",
        "Metadata:\n",
        "- Title: Membership Notice from Galveston Chamber of Commerce\n",
        "- Creator: Galveston Chamber of Commerce\n",
        "- Subject and Keywords: business communications, chamber of commerce, local trade\n",
        "- Content Description: A printed notice sent to members of the Galveston Chamber of Commerce regarding upcoming meetings and events.\n",
        "- Date: 1950-09\n",
        "- Coverage: United States - Texas - Galveston County - Galveston\n",
        "- Format: 2 pages; 28 cm; 8 x 15 cm\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical Images following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: Image title, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-scout:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"images_metadata_llama.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'images_metadata_llama.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "U7nqtIExaS02",
        "outputId": "bd393458-8dfd-42d0-8b63-673074203bbf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: Image1\n",
            "\n",
            "üìÇ Processing: Image10\n",
            "\n",
            "üìÇ Processing: Image2\n",
            "\n",
            "üìÇ Processing: Image3\n",
            "\n",
            "üìÇ Processing: Image4\n",
            "\n",
            "üìÇ Processing: Image5\n",
            "\n",
            "üìÇ Processing: Image6\n",
            "\n",
            "üìÇ Processing: Image7\n",
            "\n",
            "üìÇ Processing: Image8\n",
            "\n",
            "üìÇ Processing: Image9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_284dc879-ea55-43e2-a9e1-bab5393f2390\", \"images_metadata_llama.csv\", 6114)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'images_metadata_llama.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"Article.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Article\")):\n",
        "    base_folder = os.path.join(extract_path, \"Article\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: Newspaper Article on Cotton Trade Policies, April 12, 1943\n",
        "- Creator: The Galveston Daily News (Publisher)\n",
        "- Subject and Keywords: cotton trade, economic policies, world war II\n",
        "- Content Description: A newspaper article analyzing shifts in cotton trade regulations during wartime, focusing on Texas and Gulf Coast ports.\n",
        "- Date: 1943-04-12\n",
        "- Coverage: United States - Texas - Galveston County - Galveston\n",
        "- Format: 1 page; 28 x 22 cm\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Editorial on Agricultural Advances, May 7, 1950\n",
        "- Creator: Houston Chronicle (Publisher)\n",
        "- Subject and Keywords: agriculture, innovation, editorial opinions\n",
        "- Content Description: An editorial piece celebrating innovations in farming machinery and their impact on Texas agricultural productivity.\n",
        "- Date: 1950-05-07\n",
        "- Coverage: United States - Texas - Harris County - Houston\n",
        "- Format: 1 page; 30 x 24 cm\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical article following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: Article title, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-scout:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"articles_metadata_llama.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'articles_metadata_llama.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "ChMu-Uf6dW0l",
        "outputId": "ddc68d86-3e90-4612-fa4d-97a72839025a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: A1\n",
            "\n",
            "üìÇ Processing: a10\n",
            "\n",
            "üìÇ Processing: a11\n",
            "\n",
            "üìÇ Processing: a2\n",
            "\n",
            "üìÇ Processing: a3\n",
            "\n",
            "üìÇ Processing: a4\n",
            "\n",
            "üìÇ Processing: a5\n",
            "\n",
            "üìÇ Processing: a6\n",
            "\n",
            "üìÇ Processing: a7\n",
            "\n",
            "üìÇ Processing: a8\n",
            "\n",
            "üìÇ Processing: a9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f1b2baf8-1cec-466c-b815-84f17f2105c4\", \"articles_metadata_llama.csv\", 11654)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'articles_metadata_llama.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"Map.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Map\")):\n",
        "    base_folder = os.path.join(extract_path, \"Map\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: 1900 Map of Galveston Island Before the Hurricane\n",
        "- Creator: Texas State Cartographic Survey (Creator)\n",
        "- Subject and Keywords: maps, galveston, geographic surveys\n",
        "- Content Description: A historical map illustrating the geography and city structures of Galveston Island before the devastation of the 1900 hurricane.\n",
        "- Date: 1900\n",
        "- Coverage: United States - Texas - Galveston County - Galveston\n",
        "- Format: 1 map; 45 x 60 cm\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Trade Routes in the Gulf of Mexico, 1895\n",
        "- Creator: United States Department of Commerce (Creator)\n",
        "- Subject and Keywords: shipping routes, gulf of mexico, maritime trade\n",
        "- Content Description: A printed map highlighting major shipping and trade routes across the Gulf of Mexico region.\n",
        "- Date: 1895\n",
        "- Coverage: Gulf of Mexico\n",
        "- Format: 1 map; 60 x 75 cm\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical map following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: Map title, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-scout:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"map_metadata_llama.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'map_metadata_llama.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "Mf7-OMntki6Z",
        "outputId": "1bc6e33e-27fd-4291-d9c4-511ce7650137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: map1\n",
            "\n",
            "üìÇ Processing: map10\n",
            "\n",
            "üìÇ Processing: map11\n",
            "\n",
            "üìÇ Processing: map12\n",
            "\n",
            "üìÇ Processing: map13\n",
            "\n",
            "üìÇ Processing: map14\n",
            "\n",
            "üìÇ Processing: map15\n",
            "\n",
            "üìÇ Processing: map16\n",
            "\n",
            "üìÇ Processing: map17\n",
            "\n",
            "üìÇ Processing: map18\n",
            "\n",
            "üìÇ Processing: map19\n",
            "\n",
            "üìÇ Processing: map2\n",
            "\n",
            "üìÇ Processing: map20\n",
            "\n",
            "üìÇ Processing: map3\n",
            "\n",
            "üìÇ Processing: map4\n",
            "\n",
            "üìÇ Processing: map5\n",
            "\n",
            "üìÇ Processing: map6\n",
            "\n",
            "üìÇ Processing: map7\n",
            "\n",
            "üìÇ Processing: map8\n",
            "\n",
            "üìÇ Processing: map9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f5b70663-7f50-49ff-a6b3-404c0ec538ba\", \"map_metadata_llama.csv\", 4819)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'map_metadata_llama.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"Paper.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Paper\")):\n",
        "    base_folder = os.path.join(extract_path, \"Paper\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: Research Paper on Agricultural Export Strategies, 1947\n",
        "- Creator: Kempner, Harris L. (Author)\n",
        "- Subject and Keywords: agriculture exports, business strategies, post-war economy\n",
        "- Content Description: A typewritten research paper discussing approaches to expanding Texas agricultural exports after World War II.\n",
        "- Date: 1947-09-15\n",
        "- Coverage: United States - Texas\n",
        "- Format: 15 pages; 21 x 28 cm\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Personal Letter on Business Challenges, June 2, 1955\n",
        "- Creator: Kempner, Daniel W. (Author)\n",
        "- Subject and Keywords: personal correspondence, business hardships, family letters\n",
        "- Content Description: A personal paper discussing financial challenges and family matters in the cotton industry.\n",
        "- Date: 1955-06-02\n",
        "- Coverage: United States - Texas - Galveston County\n",
        "- Format: 3 pages; 20 x 25 cm\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical papers following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: Paper title, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-scout:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"Paper_metadata_llama.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'Paper_metadata_llama.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "CHSBfEi5pdks",
        "outputId": "33988a95-7c8f-4ab8-ddec-7e03fcce6613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: p1\n",
            "\n",
            "üìÇ Processing: p10\n",
            "\n",
            "üìÇ Processing: p11\n",
            "\n",
            "üìÇ Processing: p12\n",
            "\n",
            "üìÇ Processing: p13\n",
            "\n",
            "üìÇ Processing: p14\n",
            "\n",
            "üìÇ Processing: p2\n",
            "\n",
            "üìÇ Processing: p3\n",
            "\n",
            "üìÇ Processing: p4\n",
            "\n",
            "üìÇ Processing: p5\n",
            "\n",
            "üìÇ Processing: p6\n",
            "\n",
            "üìÇ Processing: p7\n",
            "\n",
            "üìÇ Processing: p8\n",
            "\n",
            "üìÇ Processing: p9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_abf8e076-ced3-4cb6-9d0f-e6cac60e6349\", \"Paper_metadata_llama.csv\", 9195)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'Paper_metadata_llama.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"Legislative document.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Legislative document\")):\n",
        "    base_folder = os.path.join(extract_path, \"Legislative document\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: Texas Senate Bill on Cotton Regulation, 1939\n",
        "- Creator: Texas Legislature (Author)\n",
        "- Subject and Keywords: legislation, cotton regulation, agriculture policy\n",
        "- Content Description: A legislative document outlining regulations for cotton production limits and export incentives within Texas.\n",
        "- Date: 1939-03-21\n",
        "- Coverage: United States - Texas\n",
        "- Format: 8 pages; 21 x 30 cm\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Congressional Report on Southern Agriculture, 1945\n",
        "- Creator: United States Congress (Author)\n",
        "- Subject and Keywords: congressional reports, southern agriculture, federal support\n",
        "- Content Description: A report prepared for the U.S. Congress discussing the state of agriculture in the southern states and recommending financial aid.\n",
        "- Date: 1945-07-10\n",
        "- Coverage: United States - Southern States\n",
        "- Format: 20 pages; 22 x 28 cm\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical Legislative document following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: Legislative document title, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-scout:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"Legislative document_metadata_llama.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'Legislative document_metadata_llama.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "6BQStIIEruN8",
        "outputId": "941a0124-d93b-4337-f705-159ae29c5d70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: ld1\n",
            "\n",
            "üìÇ Processing: ld10\n",
            "\n",
            "üìÇ Processing: ld11\n",
            "\n",
            "üìÇ Processing: ld12\n",
            "\n",
            "üìÇ Processing: ld13\n",
            "\n",
            "üìÇ Processing: ld14\n",
            "\n",
            "üìÇ Processing: ld15\n",
            "\n",
            "üìÇ Processing: ld2\n",
            "\n",
            "üìÇ Processing: ld3\n",
            "\n",
            "üìÇ Processing: ld4\n",
            "\n",
            "üìÇ Processing: ld5\n",
            "\n",
            "üìÇ Processing: ld6\n",
            "\n",
            "üìÇ Processing: ld7\n",
            "\n",
            "üìÇ Processing: ld8\n",
            "\n",
            "üìÇ Processing: ld9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5e1dc013-f364-4bd0-9cab-01a62526346e\", \"Legislative document_metadata_llama.csv\", 14791)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'Legislative document_metadata_llama.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"CLIPPING.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"CLIPPING\")):\n",
        "    base_folder = os.path.join(extract_path, \"CLIPPING\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: Newspaper Clipping on Cotton Price Surge, March 15, 1951\n",
        "- Creator: The Dallas Morning News (Publisher)\n",
        "- Subject and Keywords: cotton prices, market trends, news clippings\n",
        "- Content Description: A newspaper clipping reporting a significant rise in cotton prices across southern states due to increased international demand.\n",
        "- Date: 1951-03-15\n",
        "- Coverage: United States - Texas - Dallas County\n",
        "- Format: 1 clipping; 28 x 22 cm\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Clipping About Agricultural Technology Adoption, 1950\n",
        "- Creator: Houston Chronicle (Publisher)\n",
        "- Subject and Keywords: agricultural innovations, technology adoption, texas farming\n",
        "- Content Description: A clipped newspaper article showcasing how Texas farmers were adopting modern farming technologies after World War II.\n",
        "- Date: 1950-06-18\n",
        "- Coverage: United States - Texas\n",
        "- Format: 1 clipping; 30 x 24 cm\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical CLIPPINGS following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: CLIPPING title, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-scout:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"clipping_metadata_llama.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'clipping_metadata_llama.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "PHIETp16vCbe",
        "outputId": "2f2f27bf-8699-4c37-832e-5adcf7a17fd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: clipping1\n",
            "\n",
            "üìÇ Processing: clipping10\n",
            "\n",
            "üìÇ Processing: clipping11\n",
            "\n",
            "üìÇ Processing: clipping12\n",
            "\n",
            "üìÇ Processing: clipping13\n",
            "\n",
            "üìÇ Processing: clipping14\n",
            "\n",
            "üìÇ Processing: clipping15\n",
            "\n",
            "üìÇ Processing: clipping16\n",
            "\n",
            "üìÇ Processing: clipping17\n",
            "\n",
            "üìÇ Processing: clipping18\n",
            "\n",
            "üìÇ Processing: clipping19\n",
            "\n",
            "üìÇ Processing: clipping2\n",
            "\n",
            "üìÇ Processing: clipping20\n",
            "\n",
            "üìÇ Processing: clipping21\n",
            "\n",
            "üìÇ Processing: clipping22\n",
            "\n",
            "üìÇ Processing: clipping23\n",
            "\n",
            "üìÇ Processing: clipping24\n",
            "\n",
            "üìÇ Processing: clipping25\n",
            "\n",
            "üìÇ Processing: clipping26\n",
            "\n",
            "üìÇ Processing: clipping27\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6c0aa0e7-d087-4d20-ac76-db8cf92d3ad1\", \"clipping_metadata_llama.csv\", 17555)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'clipping_metadata_llama.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"Legal Document.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Legal Document\")):\n",
        "    base_folder = os.path.join(extract_path, \"Legal Document\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: Legal Agreement on Land Sale between Kempner and Associates, 1942\n",
        "- Creator: Galveston County Court (Author)\n",
        "- Subject and Keywords: land agreements, property sales, legal documentation\n",
        "- Content Description: A notarized legal document recording the terms and conditions of a land sale transaction involving members of the Kempner family.\n",
        "- Date: 1942-11-03\n",
        "- Coverage: United States - Texas - Galveston County\n",
        "- Format: 5 pages; 22 x 28 cm\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Court Order Regarding Agricultural Subsidy Dispute, 1947\n",
        "- Creator: Texas Supreme Court (Author)\n",
        "- Subject and Keywords: court rulings, agriculture subsidies, legal disputes\n",
        "- Content Description: A court order resolving a subsidy dispute related to cotton farming in Texas.\n",
        "- Date: 1947-04-21\n",
        "- Coverage: United States - Texas\n",
        "- Format: 7 pages; 21 x 28 cm\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical Legal Documents following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: Legal Document title, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-scout:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"Legal Document.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'Legal Document.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "a8kvlvxVwaU6",
        "outputId": "ceab2a4a-9850-45d0-d012-a8d90f082a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: Legal Document1\n",
            "\n",
            "üìÇ Processing: Legal Document10\n",
            "\n",
            "üìÇ Processing: Legal Document11\n",
            "\n",
            "üìÇ Processing: Legal Document12\n",
            "\n",
            "üìÇ Processing: Legal Document13\n",
            "\n",
            "üìÇ Processing: Legal Document14\n",
            "\n",
            "üìÇ Processing: Legal Document15\n",
            "\n",
            "üìÇ Processing: Legal Document16\n",
            "\n",
            "üìÇ Processing: Legal Document17\n",
            "\n",
            "üìÇ Processing: Legal Document18\n",
            "\n",
            "üìÇ Processing: Legal Document19\n",
            "\n",
            "üìÇ Processing: Legal Document2\n",
            "\n",
            "üìÇ Processing: Legal Document20\n",
            "\n",
            "üìÇ Processing: Legal Document21\n",
            "\n",
            "üìÇ Processing: Legal Document22\n",
            "\n",
            "üìÇ Processing: Legal Document23\n",
            "\n",
            "üìÇ Processing: Legal Document24\n",
            "\n",
            "üìÇ Processing: Legal Document25\n",
            "\n",
            "üìÇ Processing: Legal Document26\n",
            "\n",
            "üìÇ Processing: Legal Document27\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_da88f29d-ce6c-469b-a176-0e8c751006d1\", \"Legal Document.csv\", 18620)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'Legal Document.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"Newspaper.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Newspaper\")):\n",
        "    base_folder = os.path.join(extract_path, \"Newspaper\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: Front Page of The Galveston News, October 10, 1940\n",
        "- Creator: The Galveston News (Publisher)\n",
        "- Subject and Keywords: world war II news, local news, texas reporting\n",
        "- Content Description: A full-page newspaper featuring news articles about World War II updates, local events, and market conditions in Galveston.\n",
        "- Date: 1940-10-10\n",
        "- Coverage: United States - Texas - Galveston County\n",
        "- Format: 1 newspaper; 58 x 45 cm\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Sunday Business Section on Cotton Trade, 1949\n",
        "- Creator: Houston Chronicle (Publisher)\n",
        "- Subject and Keywords: business news, cotton industry, export trade\n",
        "- Content Description: Business section articles focused on the cotton trade industry boom post-World War II.\n",
        "- Date: 1949-05-22\n",
        "- Coverage: United States - Texas - Harris County\n",
        "- Format: 1 newspaper; 60 x 48 cm\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical Newspapers following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: Newspaper title, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-scout:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"Newspaper.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'Newspaper.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "Pecip2Y30Y_W",
        "outputId": "f73aa723-5dcb-4b3c-9cca-dcd44a89cf81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: Newspaper1\n",
            "\n",
            "üìÇ Processing: Newspaper10\n",
            "\n",
            "üìÇ Processing: Newspaper2\n",
            "\n",
            "üìÇ Processing: Newspaper3\n",
            "\n",
            "üìÇ Processing: Newspaper4\n",
            "\n",
            "üìÇ Processing: Newspaper5\n",
            "\n",
            "üìÇ Processing: Newspaper6\n",
            "\n",
            "üìÇ Processing: Newspaper7\n",
            "\n",
            "üìÇ Processing: Newspaper8\n",
            "\n",
            "üìÇ Processing: Newspaper9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_66f5bfb5-2656-4e7c-bf0b-e5eaa4f5e562\", \"Newspaper.csv\", 8659)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'Newspaper.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"Report.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Report\")):\n",
        "    base_folder = os.path.join(extract_path, \"Report\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: Annual Report on Agricultural Exports, 1945\n",
        "- Creator: United States Department of Agriculture (Author)\n",
        "- Subject and Keywords: agricultural exports, annual reports, postwar recovery\n",
        "- Content Description: A detailed report summarizing agricultural export trends and trade policies affecting Texas during the postwar period.\n",
        "- Date: 1945-12-31\n",
        "- Coverage: United States\n",
        "- Format: 45 pages; 21 x 28 cm\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Galveston Cotton Shipping Statistics Report, 1948\n",
        "- Creator: Galveston Cotton Exchange (Author)\n",
        "- Subject and Keywords: shipping statistics, cotton trade, port records\n",
        "- Content Description: A statistical report documenting the number of cotton bales shipped from the Port of Galveston during the year 1948.\n",
        "- Date: 1948-11-15\n",
        "- Coverage: United States - Texas - Galveston County - Galveston\n",
        "- Format: 25 pages; 21 x 28 cm\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical Reports following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: Report title, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-scout:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"Report.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'Report.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "pNlFdaZv3lh6",
        "outputId": "09c4437e-ce49-444e-b8c7-444cb0b38aa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: Report 1\n",
            "\n",
            "üìÇ Processing: Report 10\n",
            "\n",
            "üìÇ Processing: Report 11\n",
            "\n",
            "üìÇ Processing: Report 12\n",
            "\n",
            "üìÇ Processing: Report 13\n",
            "\n",
            "üìÇ Processing: Report 14\n",
            "\n",
            "üìÇ Processing: Report 15\n",
            "\n",
            "üìÇ Processing: Report 16\n",
            "\n",
            "üìÇ Processing: Report 17\n",
            "\n",
            "üìÇ Processing: Report 18\n",
            "\n",
            "üìÇ Processing: Report 19\n",
            "\n",
            "üìÇ Processing: Report 2\n",
            "\n",
            "üìÇ Processing: Report 20\n",
            "\n",
            "üìÇ Processing: Report 21\n",
            "\n",
            "üìÇ Processing: Report 22\n",
            "\n",
            "üìÇ Processing: Report 23\n",
            "\n",
            "üìÇ Processing: Report 24\n",
            "\n",
            "üìÇ Processing: Report 25\n",
            "\n",
            "üìÇ Processing: Report 26\n",
            "\n",
            "üìÇ Processing: Report 27\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d2978883-b5d9-41b3-a75d-3d7f66de8a2c\", \"Report.csv\", 19408)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'Report.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"Script.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Script\")):\n",
        "    base_folder = os.path.join(extract_path, \"Script\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: Script for Cotton Industry Promotional Film, 1948\n",
        "- Creator: Galveston Cotton Council (Author)\n",
        "- Subject and Keywords: cotton trade, promotional films, texas economy\n",
        "- Content Description: A scripted narration intended for a short promotional film highlighting the importance of the cotton industry in Texas during the late 1940s.\n",
        "- Date: 1948-05-01\n",
        "- Coverage: United States - Texas\n",
        "- Format: 12 pages; 21 x 28 cm\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Radio Broadcast Script on Postwar Trade, 1946\n",
        "- Creator: United States Department of Commerce (Author)\n",
        "- Subject and Keywords: radio scripts, postwar economy, trade relations\n",
        "- Content Description: A typed script for a scheduled radio broadcast discussing postwar trade expansion policies and opportunities in the cotton sector.\n",
        "- Date: 1946-08-12\n",
        "- Coverage: United States - Washington, D.C.\n",
        "- Format: 8 pages; 21 x 28 cm\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical Script following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: Script title, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-scout:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"Script.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'Script.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "s2E-2FW65JCJ",
        "outputId": "86431164-fd29-45f4-927e-187251fd54c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: script1\n",
            "\n",
            "üìÇ Processing: script10\n",
            "\n",
            "üìÇ Processing: script11\n",
            "\n",
            "üìÇ Processing: script12\n",
            "\n",
            "üìÇ Processing: script13\n",
            "\n",
            "üìÇ Processing: script14\n",
            "\n",
            "üìÇ Processing: script2\n",
            "\n",
            "üìÇ Processing: script3\n",
            "\n",
            "üìÇ Processing: script4\n",
            "\n",
            "üìÇ Processing: script5\n",
            "\n",
            "üìÇ Processing: script6\n",
            "\n",
            "üìÇ Processing: script7\n",
            "\n",
            "üìÇ Processing: script8\n",
            "\n",
            "üìÇ Processing: script9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_47075fab-920a-4171-a708-6f15783cb1a8\", \"Script.csv\", 12870)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'Script.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"Poem.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Poem\")):\n",
        "    base_folder = os.path.join(extract_path, \"Poem\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: Poem \"The Cotton Fields of Galveston\", 1938\n",
        "- Creator: Kempner, Harris L. (Poet)\n",
        "- Subject and Keywords: poetry, cotton fields, galveston\n",
        "- Content Description: A lyrical poem celebrating the beauty and toil of workers in the cotton fields surrounding Galveston during the early 20th century.\n",
        "- Date: 1938-04-10\n",
        "- Coverage: United States - Texas - Galveston County - Galveston\n",
        "- Format: 1 page; 21 x 28 cm\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: \"Songs of the Harvest\", Collection of Poems, 1942\n",
        "- Creator: Unknown (Poet)\n",
        "- Subject and Keywords: harvest poetry, seasonal change, rural life\n",
        "- Content Description: A collection of short poems reflecting on harvest time, the changing seasons, and agricultural life.\n",
        "- Date: 1942\n",
        "- Coverage: United States\n",
        "- Format: 10 pages; 21 x 28 cm\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical poems following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: poem title, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-scout:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"Poem.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'Poem.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "ZxJAWoEZ-ej3",
        "outputId": "0ec71b45-0cc7-42f2-a1bb-1ae4de65c220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: Poem 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_268c1cce-2a39-44e5-bc1f-d6b9d9281ed9\", \"Script.csv\", 795)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'Poem.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"Text.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Text\")):\n",
        "    base_folder = os.path.join(extract_path, \"Text\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: Notes on Cotton Prices in the 1940s\n",
        "- Creator: Kempner, Daniel W. (Author)\n",
        "- Subject and Keywords: cotton pricing, handwritten notes, agricultural economy\n",
        "- Content Description: Handwritten notes analyzing cotton price trends and trading volumes from 1940 to 1949, based on personal observations.\n",
        "- Date: 1949-08-20\n",
        "- Coverage: United States - Texas\n",
        "- Format: 3 pages; 20 x 25 cm\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Typed Notes on Business Strategy\n",
        "- Creator: Unknown (Author)\n",
        "- Subject and Keywords: business strategies, operational notes, historical planning\n",
        "- Content Description: Typed text outlining strategies for managing agricultural export business operations, intended for internal use.\n",
        "- Date: 1950-05-15\n",
        "- Coverage: United States\n",
        "- Format: 5 pages; 21 x 28 cm\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical texts following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: Text title, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-scout:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"Text.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'Text.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "8vuPLk8J_AOx",
        "outputId": "d929c904-294e-4b59-eae5-f710771dfa2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: Text 1\n",
            "\n",
            "üìÇ Processing: Text 10\n",
            "\n",
            "üìÇ Processing: Text 11\n",
            "\n",
            "üìÇ Processing: Text 12\n",
            "\n",
            "üìÇ Processing: Text 13\n",
            "\n",
            "üìÇ Processing: Text 14\n",
            "\n",
            "üìÇ Processing: Text 15\n",
            "\n",
            "üìÇ Processing: Text 16\n",
            "\n",
            "üìÇ Processing: Text 17\n",
            "\n",
            "üìÇ Processing: Text 18\n",
            "\n",
            "üìÇ Processing: Text 19\n",
            "\n",
            "üìÇ Processing: Text 2\n",
            "\n",
            "üìÇ Processing: Text 20\n",
            "\n",
            "üìÇ Processing: Text 21\n",
            "\n",
            "üìÇ Processing: Text 22\n",
            "\n",
            "üìÇ Processing: Text 23\n",
            "\n",
            "üìÇ Processing: Text 24\n",
            "\n",
            "üìÇ Processing: Text 25\n",
            "\n",
            "üìÇ Processing: Text 26\n",
            "\n",
            "üìÇ Processing: Text 27\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4f304ce6-34d1-433f-939d-7b2c5a86ee43\", \"Text.csv\", 10722)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'Text.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"Technical Drawing.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Technical Drawing\")):\n",
        "    base_folder = os.path.join(extract_path, \"Technical Drawing\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: Technical Blueprint of Cotton Storage Facility, 1943\n",
        "- Creator: Galveston Cotton Exchange (Engineer)\n",
        "- Subject and Keywords: cotton storage, technical drawings, architecture\n",
        "- Content Description: A technical drawing showing architectural specifications for a cotton storage warehouse built in Galveston.\n",
        "- Date: 1943-11-15\n",
        "- Coverage: United States - Texas - Galveston County\n",
        "- Format: 1 blueprint; 60 x 90 cm\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Schematic Plan for Industrial Cotton Gin, 1947\n",
        "- Creator: Texas Agricultural Engineering Department (Engineer)\n",
        "- Subject and Keywords: cotton gins, schematics, industrial design\n",
        "- Content Description: A detailed schematic of machinery layout for a modern cotton gin facility.\n",
        "- Date: 1947-06-01\n",
        "- Coverage: United States - Texas\n",
        "- Format: 1 schematic; 55 x 70 cm\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical Technical Drawings following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: Technical Drawingtitle, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-scout:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"Technical Drawing.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'Technical Drawing.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "4iUK9n8WCPtN",
        "outputId": "a91df12e-b748-494c-9124-3e094c7a8e08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: Technical Drawing 1\n",
            "\n",
            "üìÇ Processing: Technical Drawing 2\n",
            "\n",
            "üìÇ Processing: Technical Drawing 3\n",
            "\n",
            "üìÇ Processing: Technical Drawing 4\n",
            "\n",
            "üìÇ Processing: Technical Drawing 5\n",
            "\n",
            "üìÇ Processing: Technical Drawing 6\n",
            "\n",
            "üìÇ Processing: Technical Drawing 7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_981aa641-a13e-4431-9b9a-2b77d22eca1d\", \"Technical Drawing.csv\", 4100)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'Technical Drawing.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"Artwork.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Artwork\")):\n",
        "    base_folder = os.path.join(extract_path, \"Artwork\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: Painting \"Harvest Day at the Plantation\", 1935\n",
        "- Creator: Johnson, Emily S. (Artist)\n",
        "- Subject and Keywords: harvest scenes, plantation life, agricultural artwork\n",
        "- Content Description: An oil painting depicting a lively cotton harvest scene on a Texas plantation.\n",
        "- Date: 1935\n",
        "- Coverage: United States - Texas\n",
        "- Format: 1 painting; 60 x 75 cm\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Sketches of Rural Texas Life, circa 1940\n",
        "- Creator: Unknown (Artist)\n",
        "- Subject and Keywords: rural texas, sketchbook, everyday life\n",
        "- Content Description: A collection of pencil sketches illustrating daily rural life and work activities across Texas towns.\n",
        "- Date: 1940~\n",
        "- Coverage: United States - Texas\n",
        "- Format: 12 sketches; 21 x 28 cm each\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical Artworks following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: Artwork title, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-scout:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"Artwork.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'Artwork.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "2ieXZEGhDBZd",
        "outputId": "a442ef7f-776c-4cef-e9f0-408498d4a25f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: Artwork1\n",
            "\n",
            "üìÇ Processing: Artwork10\n",
            "\n",
            "üìÇ Processing: Artwork11\n",
            "\n",
            "üìÇ Processing: Artwork12\n",
            "\n",
            "üìÇ Processing: Artwork13\n",
            "\n",
            "üìÇ Processing: Artwork2\n",
            "\n",
            "üìÇ Processing: Artwork3\n",
            "\n",
            "üìÇ Processing: Artwork4\n",
            "\n",
            "üìÇ Processing: Artwork5\n",
            "\n",
            "üìÇ Processing: Artwork6\n",
            "\n",
            "üìÇ Processing: Artwork7\n",
            "\n",
            "üìÇ Processing: Artwork8\n",
            "\n",
            "üìÇ Processing: Artwork9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e8cf6cfe-076a-4a3f-8034-ef0ddbe59d0e\", \"Artwork.csv\", 7253)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'Artwork.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"Collection.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Collection\")):\n",
        "    base_folder = os.path.join(extract_path, \"Collection\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: Kempner Family Cotton Trade Correspondence Collection, 1920‚Äì1950\n",
        "- Creator: Kempner Family (Collector)\n",
        "- Subject and Keywords: business correspondence, cotton trade, family collections\n",
        "- Content Description: A curated collection of letters, contracts, and trade documents related to the cotton trade business handled by the Kempner family.\n",
        "- Date: 1920/1950\n",
        "- Coverage: United States - Texas - Galveston County - Galveston\n",
        "- Format: 200 documents; various sizes\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Collection of Galveston Agricultural Reports, 1930‚Äì1945\n",
        "- Creator: Rosenberg Library Archives (Collector)\n",
        "- Subject and Keywords: agricultural reports, galveston records, archival collections\n",
        "- Content Description: A collection of annual agricultural reports documenting the cotton, grain, and livestock industries in Galveston County.\n",
        "- Date: 1930/1945\n",
        "- Coverage: United States - Texas - Galveston County\n",
        "- Format: 50 reports; various formats\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical Collection following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: Collection title, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-scout:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"Collection.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'Collection.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "MbAg65s3EphY",
        "outputId": "4b974d09-0aec-441a-dcca-cfd78ba6108b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: Collection 1\n",
            "\n",
            "üìÇ Processing: Collection 2\n",
            "\n",
            "üìÇ Processing: Collection 3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_86a447c5-d264-43f1-b350-a2fa2f8652c4\", \"Collection.csv\", 2539)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'Collection.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"journal.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Journal_Newsletter_Magazine\")):\n",
        "    base_folder = os.path.join(extract_path, \"Journal_Newsletter_Magazine\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: Kempner News Bulletin, Vol. 1, No. 3, March 1947\n",
        "- Creator: Harris and Eliza Kempner Fund (Publisher)\n",
        "- Subject and Keywords: philanthropy news, community projects, galveston\n",
        "- Content Description: A monthly bulletin featuring updates on community donations, scholarship funds, and philanthropic events sponsored by the Kempner family.\n",
        "- Date: 1947-03-15\n",
        "- Coverage: United States - Texas - Galveston County - Galveston\n",
        "- Format: 8 pages; 22 x 28 cm\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Cotton Trade Monthly Review, May 1949\n",
        "- Creator: Kempner Cotton Exchange (Publisher)\n",
        "- Subject and Keywords: cotton market updates, business news, agricultural trade\n",
        "- Content Description: A periodical reviewing monthly cotton market statistics, pricing fluctuations, and trade news relevant to the Galveston business community.\n",
        "- Date: 1949-05-01\n",
        "- Coverage: United States - Texas - Galveston\n",
        "- Format: 12 pages; 21 x 27 cm\n",
        "\n",
        "Example 3:\n",
        "Metadata:\n",
        "- Title: Kempner Family Quarterly Newsletter, Winter 1950\n",
        "- Creator: Kempner Family Archives (Publisher)\n",
        "- Subject and Keywords: family history, community events, heritage preservation\n",
        "- Content Description: A quarterly newsletter highlighting major events, historical notes, and family gatherings related to the Kempner family.\n",
        "- Date: 1950-12-01\n",
        "- Coverage: United States - Texas\n",
        "- Format: 10 pages; 22 x 28 cm\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical Journal, Newsletter, Magazine following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: Journal_Newsletter_Magazine title, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-scout:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"Journal_Newsletter_Magazine.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'Journal_Newsletter_Magazine.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "Wl7gLf9aGa-3",
        "outputId": "fc5e285b-b622-4b35-8b24-7b963d4165c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: Letter 1\n",
            "\n",
            "üìÇ Processing: Letter 10\n",
            "\n",
            "üìÇ Processing: Letter 11\n",
            "\n",
            "üìÇ Processing: Letter 12\n",
            "\n",
            "üìÇ Processing: Letter 13\n",
            "\n",
            "üìÇ Processing: Letter 16\n",
            "\n",
            "üìÇ Processing: Letter 17\n",
            "\n",
            "üìÇ Processing: Letter 18\n",
            "\n",
            "üìÇ Processing: Letter 2\n",
            "\n",
            "üìÇ Processing: Letter 20\n",
            "\n",
            "üìÇ Processing: Letter 21\n",
            "\n",
            "üìÇ Processing: Letter 22\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_10e1d0c9-ca26-4053-8abc-7329e3fbcc2c\", \"Journal_Newsletter_Magazine.csv\", 10587)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'Journal_Newsletter_Magazine.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"postcards.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"postcards\")):\n",
        "    base_folder = os.path.join(extract_path, \"postcards\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: Postcard of Broadway Street in Galveston, 1925\n",
        "- Creator: Unknown (Photographer)\n",
        "- Subject and Keywords: galveston, street scenes, postcards\n",
        "- Content Description: A colorized postcard showing a bustling view of Broadway Street lined with historic homes and businesses in Galveston, Texas.\n",
        "- Date: 1925-07-04\n",
        "- Coverage: United States - Texas - Galveston County - Galveston\n",
        "- Format: 1 postcard; 14 x 9 cm\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Postcard Depicting Cotton Shipping Docks, 1930\n",
        "- Creator: Unknown (Photographer)\n",
        "- Subject and Keywords: cotton docks, harbor scenes, galveston shipping\n",
        "- Content Description: A vintage postcard featuring ships docked at Galveston's cotton wharves, busy with workers and cargo.\n",
        "- Date: 1930\n",
        "- Coverage: United States - Texas - Galveston County - Galveston\n",
        "- Format: 1 postcard; 14 x 9 cm\n",
        "\n",
        "Example 3:\n",
        "Metadata:\n",
        "- Title: Greeting Postcard from Galveston Beach, 1942\n",
        "- Creator: Coastal Views Publishing (Photographer)\n",
        "- Subject and Keywords: beaches, tourism, postcards\n",
        "- Content Description: A cheerful beach-themed postcard sent as a summer greeting from Galveston Beach.\n",
        "- Date: 1942-06-10\n",
        "- Coverage: United States - Texas - Galveston County - Galveston\n",
        "- Format: 1 postcard; 14 x 9 cm\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical Postcard following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: Postcard title, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-scout:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"postcards.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'postcards.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "imWcU04TOEzK",
        "outputId": "f8b23a28-2349-4866-99e0-bd8ab23fc2b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: Postcard1\n",
            "\n",
            "üìÇ Processing: Postcard10\n",
            "\n",
            "üìÇ Processing: Postcard11\n",
            "\n",
            "üìÇ Processing: Postcard12\n",
            "\n",
            "üìÇ Processing: Postcard13\n",
            "\n",
            "üìÇ Processing: Postcard14\n",
            "\n",
            "üìÇ Processing: Postcard15\n",
            "\n",
            "üìÇ Processing: Postcard16\n",
            "\n",
            "üìÇ Processing: Postcard17\n",
            "\n",
            "üìÇ Processing: Postcard18\n",
            "\n",
            "üìÇ Processing: Postcard19\n",
            "\n",
            "üìÇ Processing: Postcard2\n",
            "\n",
            "üìÇ Processing: Postcard20\n",
            "\n",
            "üìÇ Processing: Postcard21\n",
            "\n",
            "üìÇ Processing: Postcard22\n",
            "\n",
            "üìÇ Processing: Postcard23\n",
            "\n",
            "üìÇ Processing: Postcard24\n",
            "\n",
            "üìÇ Processing: Postcard25\n",
            "\n",
            "üìÇ Processing: Postcard26\n",
            "\n",
            "üìÇ Processing: Postcard27\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_91e5ffd5-d6d1-4860-8bda-50b9e30e5ee9\", \"postcards.csv\", 16333)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'postcards.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "\n",
        "# ===============================\n",
        "# üîë ENTER YOUR OPENROUTER CREDENTIALS\n",
        "# ===============================\n",
        "OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \").strip()\n",
        "REFERER = \"https://your-site-url.com\"  # Optional\n",
        "TITLE = \"UNTL Metadata Generator\"       # Optional for ranking\n",
        "\n",
        "\n",
        "zip_path = \"Physical Object.zip\"\n",
        "extract_path = \"/content/documents\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# üõ† Dynamically detect if 'Pamphlet' folder exists inside or not\n",
        "if os.path.isdir(os.path.join(extract_path, \"Physical Object\")):\n",
        "    base_folder = os.path.join(extract_path, \"Physical Object\")\n",
        "else:\n",
        "    base_folder = extract_path\n",
        "\n",
        "# ===============================\n",
        "# üõ†Ô∏è UTILITY FUNCTIONS\n",
        "# ===============================\n",
        "def resize_image(path, max_width=1024):\n",
        "    img = Image.open(path)\n",
        "    if img.width > max_width:\n",
        "        img.thumbnail((max_width, max_width))\n",
        "        img.save(path, format=\"JPEG\", quality=85)\n",
        "\n",
        "def extract_text_from_images(image_paths):\n",
        "    full_text = \"\"\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            text = pytesseract.image_to_string(Image.open(path))\n",
        "            full_text += text + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed for {path}: {e}\")\n",
        "    return full_text.strip()\n",
        "\n",
        "def parse_dublin_core_block(text_block):\n",
        "    field_map = {}\n",
        "    pattern = r\"[-‚Ä¢*]\\s*(?:\\*\\*|__)?([\\w\\s]+?)(?:\\*\\*|__)?:\\s*(.+)\"\n",
        "    for line in text_block.splitlines():\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            key, value = match.groups()\n",
        "            field_map[key.strip().lower().replace(\" \", \"_\")] = value.strip()\n",
        "    return field_map\n",
        "\n",
        "few_shot_text = \"\"\"\n",
        "Example 1:\n",
        "Metadata:\n",
        "- Title: Kempner Family Silver Trophy Cup, 1935\n",
        "- Creator: Unknown (Silversmith)\n",
        "- Subject and Keywords: trophies, silverware, family memorabilia\n",
        "- Content Description: A silver trophy cup awarded to the Kempner family for contributions to local agricultural fairs, engraved with names and dates.\n",
        "- Date: 1935\n",
        "- Coverage: United States - Texas - Galveston County\n",
        "- Format: 1 trophy; 30 cm height\n",
        "\n",
        "Example 2:\n",
        "Metadata:\n",
        "- Title: Commemorative Cotton Bale, 1940\n",
        "- Creator: Texas Cotton Growers Association (Maker)\n",
        "- Subject and Keywords: cotton samples, agricultural exhibits, memorabilia\n",
        "- Content Description: A miniature cotton bale produced for display at the Texas State Fair to showcase cotton production techniques.\n",
        "- Date: 1940\n",
        "- Coverage: United States - Texas\n",
        "- Format: 1 object; 10 x 15 x 20 cm\n",
        "\n",
        "Example 3:\n",
        "Metadata:\n",
        "- Title: Wooden Crate Used for Cotton Shipping, 1928\n",
        "- Creator: Unknown (Craftsman)\n",
        "- Subject and Keywords: shipping crates, cotton trade, physical objects\n",
        "- Content Description: A large wooden crate used for transporting cotton bales from Galveston docks to export destinations.\n",
        "- Date: 1928\n",
        "- Coverage: United States - Texas - Galveston County - Galveston\n",
        "- Format: 1 crate; 60 x 80 x 90 cm\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a metadata generator tasked with producing 7 fields for historical Physical Objects following University of North Texas Libraries Metadata Input Guidelines.\n",
        "\n",
        "FIELD OUTPUT RULES:\n",
        "- Output only the 7 Dublin Core fields in plain text (no markdown).\n",
        "- Format each field like: - Title: [value]\n",
        "- Output the fields in this exact order: Title, Creator, Contributor, Date, Content Description, Subject and Keywords, Coverage\n",
        "\n",
        "üõ† FORMATTING INSTRUCTIONS:\n",
        "- Title must include starting from resource type followed by the name sender to receiver followed by date in \"Month Day, Year\" format (Example: Physical Object, July 2025).\n",
        "- Creator and Contributor names must be in \"Last, First (Role)\" style.\n",
        "- Dates must be ISO format YYYY-MM-DD.\n",
        "- Subjects must be 3‚Äì6 keywords, lowercase unless proper nouns.\n",
        "- Coverage should be full geographic hierarchy, e.g., \"United States - Texas - Galveston County - Galveston\" and include coverage range of dates like 2025-01-02/2025-02-03.\n",
        "- If unknown, write \"Not provided.\"\n",
        "- No markdown, no extra explanation ‚Äî only plain text following example structure.\n",
        "\"\"\"\n",
        "\n",
        "# ===============================\n",
        "# ü§ñ METADATA GENERATOR USING OPENROUTER\n",
        "# ===============================\n",
        "def generate_metadata_from_text(extracted_text):\n",
        "    prompt = few_shot_text + \"\\n\\nHere is the extracted text from the document:\\n\" + extracted_text + \"\\n\\nNow write all 15 metadata fields.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\", \"content\": prompt.strip()}\n",
        "    ]\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": REFERER,\n",
        "        \"X-Title\": TITLE\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"meta-llama/llama-4-scout:free\",\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.4,\n",
        "        \"max_tokens\": 1500\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OpenRouter API Error: {e}\")\n",
        "        return \"[GPT ERROR]\"\n",
        "\n",
        "# ===============================\n",
        "# üì¶ MAIN LOOP TO PROCESS DOCUMENTS\n",
        "fields = [\"title\", \"creator\", \"contributor\", \"date\", \"content_description\", \"subject_and_keywords\", \"coverage\"]\n",
        "\n",
        "rows = []\n",
        "\n",
        "doc_folders = sorted([\n",
        "    os.path.join(base_folder, d)\n",
        "    for d in os.listdir(base_folder)\n",
        "    if os.path.isdir(os.path.join(base_folder, d))\n",
        "])[:20]\n",
        "\n",
        "for folder in doc_folders:\n",
        "    doc_id = os.path.basename(folder)\n",
        "    image_files = sorted([\n",
        "        os.path.join(folder, f)\n",
        "        for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ])[:4]\n",
        "\n",
        "    if not image_files:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÇ Processing: {doc_id}\")\n",
        "    for img in image_files:\n",
        "        resize_image(img)\n",
        "\n",
        "    extracted_text = extract_text_from_images(image_files)\n",
        "    raw_output = generate_metadata_from_text(extracted_text)\n",
        "\n",
        "    if \"[GPT ERROR]\" in raw_output:\n",
        "        row = {f: \"\" for f in fields}\n",
        "        row[\"document_id\"] = doc_id\n",
        "        row[\"description\"] = \"[Error occurred]\"\n",
        "        row[\"raw_output\"] = raw_output\n",
        "        rows.append(row)\n",
        "        continue\n",
        "\n",
        "    parsed = parse_dublin_core_block(raw_output)\n",
        "    row = {f: parsed.get(f, \"\") for f in fields}\n",
        "    row[\"document_id\"] = doc_id\n",
        "    row[\"raw_output\"] = raw_output\n",
        "    rows.append(row)\n",
        "\n",
        "# ===============================\n",
        "# üíæ EXPORT CSV\n",
        "# ===============================\n",
        "df = pd.DataFrame(rows)\n",
        "output_csv = \"Physical Object.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "files.download(output_csv)\n",
        "\n",
        "print(\"\\n‚úÖ Metadata generation completed and saved to 'Physical Object.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "MbTYBtVnPnbe",
        "outputId": "0c008872-5afa-4e42-fd61-7d1588706eeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenRouter API Key: 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\n",
            "\n",
            "üìÇ Processing: Physicalobject1\n",
            "\n",
            "üìÇ Processing: Physicalobject10\n",
            "\n",
            "üìÇ Processing: Physicalobject11\n",
            "\n",
            "üìÇ Processing: Physicalobject2\n",
            "\n",
            "üìÇ Processing: Physicalobject3\n",
            "\n",
            "üìÇ Processing: Physicalobject4\n",
            "\n",
            "üìÇ Processing: Physicalobject5\n",
            "\n",
            "üìÇ Processing: Physicalobject6\n",
            "\n",
            "üìÇ Processing: Physicalobject7\n",
            "\n",
            "üìÇ Processing: Physicalobject8\n",
            "\n",
            "üìÇ Processing: Physicalobject9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_00527166-72ce-4980-b72f-e77860225d6f\", \"Physical Object.csv\", 6420)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Metadata generation completed and saved to 'Physical Object.csv'\n"
          ]
        }
      ]
    }
  ]
}